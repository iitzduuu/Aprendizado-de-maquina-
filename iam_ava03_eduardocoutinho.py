# -*- coding: utf-8 -*-
"""IAM_AVA03_eduardocoutinho.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wynx4WljAdDykB5UIhq5jEHv7oyEHQQ5
"""

!pip install ucimlrepo
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import seaborn as sns

from ucimlrepo import fetch_ucirepo

wine = fetch_ucirepo(id=109)

X = wine.data.features

y = wine.data.targets

print(wine.metadata)

print(wine.variables)

import pandas as pd
data_url = 'https://archive.ics.uci.edu/static/public/109/data.csv'
df = pd.read_csv(data_url)
df['quality'] = df['class'].apply(lambda x: 1 if x > 5 else 0) #cria a coluna de dados quality para fazer a comparação
X = df.drop(columns=["class", "quality"])
y = df["quality"]
print(df.columns)
df.head(20)

#teste
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # dfvide os dados em conjuntos de treino e teste

model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)), #compila o modelo
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

print(f"Acurácia no conjunto de teste: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l2

input_dim = X_train.shape[1]
model = Sequential()
model.add(Dense(64, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01)))       #bloco para previnir oferfittng
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#importa as bibliotecas dnv
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler


data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  #dados de exemplo

scaler = MinMaxScaler()

# ajustando os dados
normalized_data = scaler.fit_transform(data)

print("dados originais:")
print(data)

print("\dados normalizados:")
print(normalized_data)

X = np.random.rand(100, 10) #gera dados de exemplo
y = np.random.randint(0, 2, 100)

scaler = StandardScaler() #normaliza os dadoos
X_scaled = scaler.fit_transform(X)

kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # configurando o kfold usando 10 dobras

model = RandomForestClassifier(random_state=42)

param_dist = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=kfold, scoring='accuracy', random_state=42, n_jobs=-1)
random_search.fit(X_scaled, y)

best_model = random_search.best_estimator_

results = cross_val_score(best_model, X_scaled, y, cv=kfold, scoring='accuracy') # avaliação do modelo

print(f"melhores parâmetros: {random_search.best_params_}")
print(f"acurácia das dobras: {results}")
print(f"média de acurácia: {results.mean()}")

import matplotlib.pyplot as plt
import seaborn as sns

96
results = cross_val_score(best_model, X_scaled, y, cv=kfold, scoring='accuracy')

plt.figure(figsize=(10, 6))
sns.barplot(x=list(range(1, len(results) + 1)), y=results, palette='viridis')
plt.xlabel('número da Dobra')
plt.ylabel('acurácia')
plt.title('acurácia das Dobras da Validação Cruzada')
plt.show()

from sklearn.metrics import roc_curve, auc

val_preds = best_model.predict(X_scaled)
fpr, tpr, _ = roc_curve(y, val_preds)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('taxa de falsos positivos')
plt.ylabel('taxa de verdadeiros positivos')
plt.title('receiver operating characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

y_pred = best_model.predict(X_scaled)
cm = confusion_matrix(y, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues')
plt.xlabel('vrevisões')
plt.ylabel('valores Reais')
plt.title('matriz de Confusão')
plt.show()